{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# !pip install dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import einops\n",
    "import pandas as pd\n",
    "# import datasets\n",
    "from datasets import Dataset, load_dataset, DatasetDict\n",
    "\n",
    "class Attn(nn.Module):\n",
    "    def __init__(self, n_heads, d_embed, d_hidden):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_lin = nn.Linear(in_features=d_hidden, out_features=d_embed)\n",
    "\n",
    "        Q = nn.Linear(in_features=d_embed, out_features=d_hidden)\n",
    "        nn.init.xavier_uniform()\n",
    "\n",
    "# a = Attn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.2481, 0.6660, 0.8065],\n",
       "          [0.0584, 0.1824, 0.4292],\n",
       "          [0.7928, 0.0533, 0.6161],\n",
       "          [0.9786, 0.0642, 0.2615],\n",
       "          [0.7678, 0.6759, 0.9963]]]),\n",
       " torch.Size([1, 5, 3]))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# N, L, D, C = 1, 5, 3, 1\n",
    "# N, C, L, D = 1, 1, 5, 3\n",
    "N, L, D = 1, 5, 3\n",
    "# sample_batched_input = torch.rand((N, C, L, D))\n",
    "sample_batched_input = torch.rand((N, L, D))\n",
    "\n",
    "sample_batched_input, sample_batched_input.shape#.unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 64])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Linear(in_features=3, out_features=64)(sample_batched_input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.BatchNorm2d(affine=False, num_features=1)(\n",
    "#     nn.Linear(in_features=3, out_features=64)(sample_batched_input)\n",
    "#     ).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0.],\n",
       "          [0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.],\n",
       "          [0., 0., 0.]]]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.BatchNorm2d(affine=False, num_features=1)(torch.ones((2, 1, 2, 3)))\n",
    "torch.nn.BatchNorm2d(affine=False, num_features=1)(torch.ones((2, 1, 2, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (0): Linear(in_features=3, out_features=64, bias=True)\n",
       "  (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU()\n",
       "  (3): Dropout(p=0.0, inplace=False)\n",
       "  (4): Linear(in_features=64, out_features=128, bias=True)\n",
       "  (5): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (6): ReLU()\n",
       "  (7): Dropout(p=0.0, inplace=False)\n",
       "  (8): Linear(in_features=128, out_features=1024, bias=True)\n",
       "  (9): Dropout(p=0.0, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.ops.MLP(in_channels=3, \n",
    "                    hidden_channels=[64, 128, 1024], \n",
    "                    norm_layer=lambda x : nn.BatchNorm1d(num_features=3),#torch.nn.GroupNorm(),#(num_features=3, affine=False), \n",
    "                    activation_layer=nn.ReLU)#.forward(input=sample_batched_input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        features = [in_features] + hidden_features\n",
    "\n",
    "        # self.mlp = nn.ModuleList([\n",
    "        #     # Layer\n",
    "        #     nn.Sequential(\n",
    "        #         nn.Linear(in_features=features[i], \n",
    "        #                   out_features=features[i+1]),\n",
    "        #         nn.BatchNorm1d(num_features=3, affine=False),\n",
    "        #         nn.ReLU(),\n",
    "        #         nn.Dropout(p=0),\n",
    "        #     )\n",
    "        # self.mlp = nn.Sequential(nn.ModuleList([\n",
    "        #     # Layer\n",
    "        #     nn.Sequential(\n",
    "        #         nn.Linear(in_features=features[i], \n",
    "        #                   out_features=features[i+1]),\n",
    "        #         nn.BatchNorm1d(num_features=features[i], affine=False),\n",
    "        #         nn.ReLU(),\n",
    "        #         nn.Dropout(p=0),\n",
    "        #     )\n",
    "        # for i in range(0, len(features)-1)]))\n",
    "\n",
    "        self.mlp_layers = nn.ModuleList([\n",
    "            # Layer\n",
    "            nn.Sequential(\n",
    "                nn.Linear(in_features=features[i], \n",
    "                          out_features=features[i+1]),\n",
    "                # nn.BatchNorm1d(num_features=features[i], affine=False),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(p=0),\n",
    "            )\n",
    "        for i in range(0, len(features)-1)])\n",
    "        \n",
    "        # self.lin_1 = nn.Linear(in_features=in_features, out_features=hidden_features[0])\n",
    "        # self.lin_2 = nn.Linear\n",
    "    def forward(self, x):\n",
    "        for layer in self.mlp_layers:\n",
    "            x = layer(x)\n",
    "        return self.mlp_layers(x)\n",
    "\n",
    "# m = MLP(in_features=3, hidden_features=[64, 128, 1024])\n",
    "# m(sample_batched_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (0): Linear(in_features=3, out_features=64, bias=True)\n",
       "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU()\n",
       "  (3): Dropout(p=0.0, inplace=False)\n",
       "  (4): Linear(in_features=64, out_features=128, bias=True)\n",
       "  (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (6): ReLU()\n",
       "  (7): Dropout(p=0.0, inplace=False)\n",
       "  (8): Linear(in_features=128, out_features=1024, bias=True)\n",
       "  (9): Dropout(p=0.0, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.ops.MLP(in_channels=3, \n",
    "                    hidden_channels=[64, 128, 1024], \n",
    "                    norm_layer=nn.BatchNorm2d,#nn.RMSNorm,#lambda x : nn.BatchNorm1d(num_features=3),#torch.nn.GroupNorm(),#(num_features=3, affine=False), \n",
    "                    activation_layer=nn.ReLU)#(sample_batched_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class View(nn.Module):\n",
    "#     def __init__(self, shape):\n",
    "#         self.shape = shape\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return x.view(*self.shape)\n",
    "\n",
    "    \n",
    "class MLP_2(nn.Module):\n",
    "    class TransposeChannelSequennce(nn.Module):\n",
    "        def __init__(self, to_transpose=(-1, -2)):\n",
    "            super().__init__()\n",
    "            self.to_transpose = to_transpose\n",
    "\n",
    "        def forward(self, x:torch.Tensor):\n",
    "            return x.transpose(-1, -2)#**self.to_transpose)#-1, -2)\n",
    "    def __init__(self, in_features, hidden_features, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        features = [in_features] + hidden_features + [in_features]\n",
    "\n",
    "        self.mlp_layers = nn.ModuleList([\n",
    "            # Layer\n",
    "            nn.Sequential(\n",
    "                nn.Linear(in_features=features[i-1], \n",
    "                          out_features=features[i]),\n",
    "                TransposeChannelSequennce(),\n",
    "                nn.BatchNorm1d(num_features=features[i], affine=False),\n",
    "                TransposeChannelSequennce(),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(p=0),\n",
    "            ) if i < len(features)-1 else \n",
    "            nn.Sequential(\n",
    "                nn.Linear(in_features=features[i-1], \n",
    "                          out_features=features[i]),\n",
    "            )\n",
    "        for i in range(1, len(features))])\n",
    "        \n",
    "        # self.lin_1 = nn.Linear(in_features=in_features, out_features=hidden_features[0])\n",
    "        # self.lin_2 = nn.Linear\n",
    "    def forward(self, x):\n",
    "        for layer in self.mlp_layers:\n",
    "            x = layer(x)\n",
    "        return x#self.mlp_layers(x)\n",
    "\n",
    "# m = MLP(in_features=3, hidden_features=[64, 128, 1024])\n",
    "# m(sample_batched_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TransposeChannelSequennce' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mMLP_2\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m(sample_batched_input)\n",
      "Cell \u001b[0;32mIn[78], line 21\u001b[0m, in \u001b[0;36mMLP_2.__init__\u001b[0;34m(self, in_features, hidden_features, *args, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     19\u001b[0m features \u001b[38;5;241m=\u001b[39m [in_features] \u001b[38;5;241m+\u001b[39m hidden_features \u001b[38;5;241m+\u001b[39m [in_features]\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_layers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\u001b[43m[\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Layer\u001b[39;49;00m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mout_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mTransposeChannelSequennce\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBatchNorm1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maffine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mTransposeChannelSequennce\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mReLU\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mout_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m)\n",
      "Cell \u001b[0;32mIn[78], line 26\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     19\u001b[0m features \u001b[38;5;241m=\u001b[39m [in_features] \u001b[38;5;241m+\u001b[39m hidden_features \u001b[38;5;241m+\u001b[39m [in_features]\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_layers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# Layer\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m     24\u001b[0m         nn\u001b[38;5;241m.\u001b[39mLinear(in_features\u001b[38;5;241m=\u001b[39mfeatures[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \n\u001b[1;32m     25\u001b[0m                   out_features\u001b[38;5;241m=\u001b[39mfeatures[i]),\n\u001b[0;32m---> 26\u001b[0m         \u001b[43mTransposeChannelSequennce\u001b[49m(),\n\u001b[1;32m     27\u001b[0m         nn\u001b[38;5;241m.\u001b[39mBatchNorm1d(num_features\u001b[38;5;241m=\u001b[39mfeatures[i], affine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m     28\u001b[0m         TransposeChannelSequennce(),\n\u001b[1;32m     29\u001b[0m         nn\u001b[38;5;241m.\u001b[39mReLU(),\n\u001b[1;32m     30\u001b[0m         nn\u001b[38;5;241m.\u001b[39mDropout(p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m     31\u001b[0m     ) \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(features)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \n\u001b[1;32m     32\u001b[0m     nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m     33\u001b[0m         nn\u001b[38;5;241m.\u001b[39mLinear(in_features\u001b[38;5;241m=\u001b[39mfeatures[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \n\u001b[1;32m     34\u001b[0m                   out_features\u001b[38;5;241m=\u001b[39mfeatures[i]),\n\u001b[1;32m     35\u001b[0m     )\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(features))])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TransposeChannelSequennce' is not defined"
     ]
    }
   ],
   "source": [
    "MLP_2(in_features=D, hidden_features=[64, 128, 1024])(sample_batched_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0104, 0.9589, 0.0847],\n",
       "         [0.6557, 0.1664, 0.9013],\n",
       "         [0.2181, 0.5006, 0.2660],\n",
       "         [0.6849, 0.1917, 0.3040],\n",
       "         [0.7638, 0.0653, 0.5368]]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nn.init.xavier_uniform(torch.zeros((1, 2)))\n",
    "# \n",
    "# TransposeChannelSequennce()(TransposeChannelSequennce()(sample_batched_input)) #- sample_batched_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class View(nn.Module):\n",
    "#     def __init__(self, shape):\n",
    "#         self.shape = shape\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return x.view(*self.shape)\n",
    "\n",
    "    \n",
    "class MLP_3(nn.Module):\n",
    "    class TransposeChannelSequennce(nn.Module):\n",
    "        def __init__(self, to_transpose=(-1, -2)):\n",
    "            super().__init__()\n",
    "            self.to_transpose = to_transpose\n",
    "\n",
    "        def forward(self, x:torch.Tensor):\n",
    "            # print(\"doing tranpsose \", x.shape)\n",
    "            return x.transpose(-1, -2)#**self.to_transpose)#-1, -2)\n",
    "        #reul_at_end\n",
    "    def __init__(self, in_features, hidden_features, relu_at_end=True, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.relu_at_end = relu_at_end\n",
    "        features = [in_features] + hidden_features# + [in_features]\n",
    "\n",
    "        self.mlp_layers = nn.ModuleList([\n",
    "            # Layer\n",
    "            nn.Sequential(\n",
    "                nn.Linear(in_features=features[i-1], \n",
    "                          out_features=features[i]),\n",
    "                self.TransposeChannelSequennce(),\n",
    "                nn.BatchNorm1d(num_features=features[i], affine=False),\n",
    "                self.TransposeChannelSequennce(),\n",
    "                nn.ReLU() if i < len(features)-1 or relu_at_end else nn.Identity(),\n",
    "                nn.Dropout(p=0),\n",
    "            ) \n",
    "        for i in range(1, len(features))])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.mlp_layers:\n",
    "            x = layer(x)\n",
    "        return x#self.mlp_layers(x)\n",
    "# torch.Tensor.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0000, 0.5756, 0.7706, 1.0622],\n",
       "          [0.0000, 0.0000, 0.0000, 1.0143],\n",
       "          [0.9853, 0.0000, 0.8041, 0.0000],\n",
       "          [1.2562, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 1.5638, 0.6488, 0.2108]]], grad_fn=<ReluBackward0>),\n",
       " torch.Size([1, 4]),\n",
       " torch.Size([1, 5, 4]))"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_samp = MLP_3(in_features=3, hidden_features=[64, 128, 4])(sample_batched_input)\n",
    "mlp_samp, torch.nn.AdaptiveMaxPool1d(output_size=1)(mlp_samp.transpose(-1, -2)).squeeze(dim=-1).shape, mlp_samp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp_samp = MLP_3(in_features=3, hidden_features=[64, 128, 4])(sample_batched_input)\n",
    "# mlp_samp, torch.nn.AdaptiveMaxPool1d(output_size=1)(mlp_samp.transpose(-1, -2)), mlp_samp.shape\n",
    "\n",
    "\n",
    "class T2Net(nn.Module):\n",
    "    def __init__(self, dim=3, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.mlp = MLP_3(in_features=dim, hidden_features=[64, 128, 1024])\n",
    "        self.affine_T = nn.Linear(in_features=dim, out_features=dim, bias=True)\n",
    "        self.global_max_pool = nn.AdaptiveMaxPool1d(output_size=1)\n",
    "        \n",
    "        # self.global_max_pool = nn.AdaptiveMaxPool1d(output_size=1)\n",
    "\n",
    "    \n",
    "        # Initialize the matrix in layer as identity\n",
    "        with torch.no_grad():\n",
    "            identity_weights = torch.eye(self.affine_T.weight.shape[0], dtype=float)\n",
    "            self.affine_T.weight.copy_(identity_weights)\n",
    "            \n",
    "            # self.affine_T.bias.copy_(identity_weights)\n",
    "            \n",
    "        print(identity_weights)\n",
    "        print(self.affine_T.weight)\n",
    "        print(self.affine_T.bias)\n",
    "        # nn.Linear\n",
    "\n",
    "    \"\"\"\n",
    "    inp.shape: [N x L x D] , where N is batch, L is # of points, D is point-cloud dimension (dim of each point)\n",
    "    \"\"\"\n",
    "    def forward(self, inp):\n",
    "        return self.affine_T(inp) # TODO stub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 0., 0., 0., 1., 0., 0., 0., 1.], dtype=torch.float64)\n",
      "Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 0., 0., 0., 1., 0., 0., 0., 1.], requires_grad=True)\n",
      "first output:\n",
      " tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class TNet(nn.Module):\n",
    "    def __init__(self, dim=3, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.D = dim\n",
    "        self.mlp = MLP_3(in_features=dim, hidden_features=[64, 128, 1024])\n",
    "        self.affine_T = nn.Linear(in_features=1024, out_features=dim**2, bias=True)\n",
    "        self.global_max_pool = nn.AdaptiveMaxPool1d(output_size=1)\n",
    "\n",
    "        \n",
    "        # self.global_max_pool = nn.AdaptiveMaxPool1d(output_size=1)\n",
    "\n",
    "    \n",
    "        # Initialize the matrix in layer as identity\n",
    "        with torch.no_grad():\n",
    "            identity_bias = torch.eye(dim, dtype=float).flatten()\n",
    "            self.affine_T.bias.copy_(identity_bias)\n",
    "            self.affine_T.weight.copy_(torch.zeros_like(self.affine_T.weight))\n",
    "            \n",
    "            # self.affine_T.bias.copy_(identity_weights)\n",
    "            \n",
    "        print(identity_bias)\n",
    "        print(self.affine_T.weight)\n",
    "        print(self.affine_T.bias)\n",
    "        # nn.Linear\n",
    "\n",
    "    \"\"\"\n",
    "    inp.shape: [N x L x D] , where N is batch, L is # of points, D is point-cloud dimension (dim of each point)\n",
    "    \"\"\"\n",
    "    def forward(self, inp):\n",
    "        inp = self.mlp(inp)\n",
    "        inp = self.global_max_pool(inp.transpose(-1, -2)).squeeze(dim=-1)\n",
    "        inp = self.affine_T(inp)\n",
    "        inp = inp.reshape((self.D, self.D))#(self.affine_T.in_features, self.affine_T.in_features))\n",
    "        return inp\n",
    "        # return self.affine_T(inp) # TODO stub\n",
    "    \n",
    "# class \n",
    "    \n",
    "t = TNet(dim=3)\n",
    "tnet_out_samp = t(sample_batched_input)#(torch.tensor([1, 2, 3, 4]).float())\n",
    "\n",
    "class PointNet(nn.Module):\n",
    "    def __init__(self, dim=3, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.T1 = TNet(dim=dim)#3)\n",
    "        self.T2 = TNet(dim=64)\n",
    "\n",
    "        self.mlp1 = MLP_3(in_features=dim, hidden_features=[64, 64])\n",
    "        self.mlp2 = MLP_3(in_features=64, hidden_features=[64, 128, 1024])\n",
    "\n",
    "\n",
    "        # self.MLP_1 = torch.\n",
    "\n",
    "        self.global_max_pool = nn.AdaptiveMaxPool1d(output_size=1)\n",
    "\n",
    "    def forward(self, x):#, get_intermediate_for_segmentation=False):\n",
    "        # First transform\n",
    "        t_affine_1 = self.T1(x)\n",
    "        x = torch.matmul(x, t_affine_1)\n",
    "\n",
    "        # First mlp layer\n",
    "        x = self.mlp1(x)\n",
    "\n",
    "        # Second transform\n",
    "        t_affine_2 = self.T2(x)\n",
    "        x = torch.matmul(x, t_affine_2)\n",
    "        t_affine_2_transformed = x.clone()\n",
    "\n",
    "        # Second mlp + max pooling\n",
    "        x = self.mlp2(x)\n",
    "        # mlp2_out = x.clone()\n",
    "        # x = self.global_max_pool(x.transpose(-1, -2)).squeeze(dim=-1)\n",
    "        x = self.global_max_pool(torch.einsum('...ij->...ji', x)).squeeze(dim=-1)\n",
    "\n",
    "        return {'maxpooled_out':x, 'intermediate_affine_out':t_affine_2_transformed}#'intermediate_mlp':mlp2_out}\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# nn.MLP(n_hidden=9)\n",
    "\n",
    "def identity_matrix(dim):\n",
    "    return torch.eye(dim)\n",
    "\n",
    "\n",
    "# class GlobalMaxPoll(nn.Module):\n",
    "#     def __init__(self, *args, **kwargs) -> None:\n",
    "#         super().__init__(*args, **kwargs)\n",
    "print(\"first output:\\n\", tnet_out_samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PointNetForSegmentation(nn.Module):\n",
    "    def __init__(self, num_classes, dim=3, pointnet=None, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.point_net = pointnet if pointnet else PointNet(dim=dim)\n",
    "        self.mlp1 = MLP_3(in_features=1088, hidden_features=[512, 256, 128])\n",
    "        self.mlp2 = MLP_3(in_features=128, hidden_features=[num_classes], relu_at_end=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        point_net_out = self.point_net(x)\n",
    "        # batch,n_points = x.shape[0], x.shape[1]\n",
    "        n_points = x.shape[1]\n",
    "\n",
    "        concatted = torch.concat(\n",
    "            (point_net_out['intermediate_affine_out'], einops.repeat(point_net_out['maxpooled_out'], 'b h -> b n h', n=n_points)),#torch.tile(point_net_out['maxpooled_out'].unsqueeze(), (1, n_points, ))), #.tile((n_points, ))\n",
    "            dim=-1\n",
    "        )\n",
    "        # print(concatted)\n",
    "\n",
    "        concatted = self.mlp1(concatted)\n",
    "        concatted = self.mlp2(concatted)\n",
    "\n",
    "        return concatted\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 0., 0., 0., 1., 0., 0., 0., 1.], dtype=torch.float64)\n",
      "Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 0., 0., 0., 1., 0., 0., 0., 1.], requires_grad=True)\n",
      "tensor([1., 0., 0.,  ..., 0., 0., 1.], dtype=torch.float64)\n",
      "Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 0., 0.,  ..., 0., 0., 1.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "point_net = PointNet(dim=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.6041, 0.1600, 0.0000, 1.6762, 1.0783, 1.3375, 1.6867, 0.0000,\n",
       "          0.0000, 1.9546, 0.0000, 0.0000, 1.1513, 0.8312, 0.0000, 0.0024,\n",
       "          0.4423, 0.0000, 1.2445, 0.7504, 1.6052, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.8367, 0.0000, 0.0000, 0.0000, 0.4787,\n",
       "          0.0000, 0.0000, 0.2276, 0.9936, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.2538, 1.5712, 0.3534, 0.9825, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.9495, 0.0000, 0.0000, 0.0000, 0.2337, 0.0822, 0.0000, 0.0000,\n",
       "          1.2649, 1.4419, 0.0000, 0.0000, 0.0451, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2067, 0.0000, 1.2395, 0.4682, 1.1236, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 1.2128, 0.3529, 0.0000, 0.0000, 1.8950, 1.7297,\n",
       "          0.0000, 0.2871, 0.6073, 0.0000, 0.3994, 1.6820, 0.7648, 0.0000,\n",
       "          1.9272, 1.5931, 0.0000, 0.0000, 0.2716, 0.0000, 1.4324, 1.3473,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.5164, 0.0000, 0.0251, 0.0000,\n",
       "          0.0000, 0.7343, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.5646,\n",
       "          0.1317, 0.0093, 0.0058, 1.1472, 1.7227, 0.0000, 0.0000, 0.0000,\n",
       "          1.0525, 0.0000, 1.4136, 1.0884, 1.4268, 0.0000, 1.2924, 1.8356],\n",
       "         [0.0000, 0.0000, 1.1308, 0.0000, 0.0000, 0.0000, 0.0000, 0.4568,\n",
       "          0.9798, 0.0000, 0.4769, 0.7391, 0.0000, 0.0331, 0.0000, 0.2404,\n",
       "          0.0000, 0.3480, 0.1456, 0.7715, 0.0000, 0.1631, 0.0000, 0.8583,\n",
       "          0.0000, 0.0000, 1.9137, 0.0000, 0.0000, 0.0940, 0.2186, 0.0000,\n",
       "          0.5575, 1.6546, 0.5556, 0.0000, 0.0000, 0.0000, 0.7952, 1.1884,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.9836, 0.6930, 1.6972, 0.1649,\n",
       "          0.0000, 0.8736, 0.7343, 0.6575, 0.0000, 0.0173, 0.2910, 0.0000,\n",
       "          0.0000, 0.0000, 0.8253, 0.0843, 0.3503, 0.2935, 0.0000, 0.0000],\n",
       "         [0.0000, 1.7703, 0.0000, 0.0000, 0.0000, 0.0000, 0.1152, 0.1747,\n",
       "          0.7885, 0.0000, 0.6070, 0.0000, 0.0000, 0.2066, 0.0000, 0.0000,\n",
       "          0.0000, 1.5272, 0.0000, 0.0000, 0.0000, 0.0000, 1.3930, 0.8534,\n",
       "          0.0000, 0.0000, 0.0000, 1.4862, 0.0000, 0.8107, 0.0000, 0.0000,\n",
       "          0.9802, 0.1929, 0.1983, 0.0000, 1.6219, 1.9331, 1.2547, 1.1012,\n",
       "          0.0000, 0.0000, 1.4776, 0.0805, 0.8819, 1.1382, 0.5626, 0.0000,\n",
       "          0.0000, 0.0000, 1.4382, 0.5304, 0.0000, 1.6927, 0.0000, 1.9621,\n",
       "          0.0000, 0.3022, 0.0000, 1.0671, 0.0000, 1.8085, 0.8469, 0.0306],\n",
       "         [0.1428, 0.0000, 0.0000, 0.0000, 0.0000, 1.0014, 0.2183, 1.2331,\n",
       "          0.5595, 0.0000, 0.0000, 1.0765, 1.2937, 0.8226, 0.0000, 0.0000,\n",
       "          1.6662, 0.0000, 0.0000, 0.8378, 0.1165, 0.0000, 0.0000, 0.5713,\n",
       "          0.0000, 0.1900, 0.0000, 0.0000, 1.7346, 0.9716, 0.3856, 0.3136,\n",
       "          0.6716, 0.0000, 0.9442, 1.4026, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          1.8029, 0.0000, 0.2667, 1.2011, 0.5213, 0.4476, 0.0000, 0.0921,\n",
       "          1.0720, 1.1585, 0.0000, 0.0000, 0.0000, 0.0000, 1.7978, 0.0000,\n",
       "          0.0000, 0.2474, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
       "       grad_fn=<CloneBackward0>)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "point_net(sample_batched_input)['intermediate_affine_out']#.shape#intermediate_mlp'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3., grad_fn=<LinalgVectorNormBackward0>)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"testing regularization term\"\"\"\n",
    "torch.linalg.matrix_norm(\n",
    "            torch.eye(n=t.affine_T.weight.shape[0])\n",
    "            -\n",
    "            torch.matmul(t.affine_T.weight, t.affine_T.weight.T)\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 0., 0., 0., 1., 0., 0., 0., 1.], dtype=torch.float64)\n",
      "Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 0., 0., 0., 1., 0., 0., 0., 1.], requires_grad=True)\n",
      "tensor([1., 0., 0.,  ..., 0., 0., 1.], dtype=torch.float64)\n",
      "Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 0., 0.,  ..., 0., 0., 1.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "pointnet_seg = PointNetForSegmentation(dim=3, num_classes=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 5, 64]), torch.Size([1, 5120]))"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "point_net(sample_batched_input)['intermediate_affine_out'].shape, torch.tile(point_net(sample_batched_input)['maxpooled_out'], (1, 5)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.7522, 1.4407, 1.4489,  ..., 0.7545, 1.3683, 1.0689],\n",
       "         [0.7522, 1.4407, 1.4489,  ..., 0.7545, 1.3683, 1.0689],\n",
       "         [0.7522, 1.4407, 1.4489,  ..., 0.7545, 1.3683, 1.0689],\n",
       "         [0.7522, 1.4407, 1.4489,  ..., 0.7545, 1.3683, 1.0689],\n",
       "         [0.7522, 1.4407, 1.4489,  ..., 0.7545, 1.3683, 1.0689]]],\n",
       "       grad_fn=<ExpandBackward0>)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install einops\n",
    "import einops\n",
    "einops.repeat(point_net(sample_batched_input)['maxpooled_out'], 'b c  -> b k c', k=5)#.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.0878,  1.1475,  0.8187,  0.4445,  0.4567,  0.6572,  0.5177,\n",
       "           -0.5355],\n",
       "          [-1.7178,  1.0610,  0.6203, -1.6777,  0.6437, -0.9521, -1.3151,\n",
       "           -0.5272],\n",
       "          [ 0.5832, -0.1142, -0.2716, -0.3361, -1.4137,  0.1327, -0.0087,\n",
       "           -1.1918],\n",
       "          [-0.0793, -0.6281, -1.8466,  1.3517,  1.2358, -1.2662, -0.7538,\n",
       "            1.6136],\n",
       "          [ 1.3017, -1.4661,  0.6791,  0.2176, -0.9224,  1.4284,  1.5599,\n",
       "            0.6410]]], grad_fn=<TransposeBackward0>),\n",
       " torch.Size([1, 5, 8]))"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pointnet_segout_one =  pointnet_seg(sample_batched_input)#pointnet_seg(sample_batched_input).shape, pointnet_seg(sample_batched_input)\n",
    "pointnet_segout_one, pointnet_segout_one.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.4827, grad_fn=<NllLoss2DBackward0>)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# f = torch.eye(5).unsqueeze(0).expand((1, 5, 8))\n",
    "pointnet_segout_one = pointnet_segout_one#.flatten(-1)\n",
    "label_path = torch.ones_like(pointnet_segout_one)\n",
    "# nn.CrossEntropyLoss()(pointnet_segout_one, torch.softmax(pointnet_segout_one, dim=-1))\n",
    "# nn.CrossEntropyLoss()(pointnet_segout_one, nn.LogSoftmax(dim=-1)(pointnet_segout_one))\n",
    "# nn.CrossEntropyLoss()(pointnet_segout_one, torch.randint(size=(1, 5), high=7))\n",
    "nn.CrossEntropyLoss(reduction='mean')(pointnet_segout_one.transpose(-1,-2), torch.randint(size=(1, 5), high=7))\n",
    "\n",
    "\n",
    "\n",
    "# torch.softmax(pointnet_segout_one, dim=-1)\n",
    "# nn.LogSoftmax(dim=-1)(pointnet_segout_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2703,), (2703, 3))"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"dataset loading\"\"\"\n",
    "# pd.read_csv()\n",
    "# with open(\"/Users/ishanshastri/Desktop/3D/pointnet/bag_dataset/expert_verified/points_label/1b9ef45fefefa35ed13f430b2941481.seg\",\"r\") as f:\n",
    "#     loaded_list = f.read()\n",
    "# loaded_tensor = torch.tensor(eval(loaded_list))\n",
    "\n",
    "from numpy import genfromtxt\n",
    "genfromtxt(\"/Users/ishanshastri/Desktop/3D/pointnet/bag_dataset/expert_verified/points_label/1b9ef45fefefa35ed13f430b2941481.seg\").shape,\\\n",
    "genfromtxt(\"/Users/ishanshastri/Desktop/3D/pointnet/bag_dataset/points/1b9ef45fefefa35ed13f430b2941481.pts\").shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 1., 1., 0.])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genfromtxt(\"/Users/ishanshastri/Desktop/3D/pointnet/bag_dataset/expert_verified/points_label/1b9ef45fefefa35ed13f430b2941481.seg\") - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "points_path = r'/Users/ishanshastri/Desktop/3D/pointnet/bag_dataset/points'\n",
    "labels_path = r'/Users/ishanshastri/Desktop/3D/pointnet/bag_dataset/expert_verified/points_label'\n",
    "import os\n",
    "\n",
    "arr_x, arr_y = [], []\n",
    "\n",
    "for fname in sorted(os.listdir(points_path)):\n",
    "    x_path = os.path.join(points_path, fname)\n",
    "    arr_x.append(genfromtxt(x_path))\n",
    "\n",
    "for fname in sorted(os.listdir(labels_path)):\n",
    "    label_path = os.path.join(labels_path, fname)\n",
    "    arr_y.append(genfromtxt(label_path)-1)\n",
    "\n",
    "# arr_x, arr_y = np.array(arr_x), np.array(arr_y)\n",
    "# arr_x = np.array(arr_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "print(re.match(\"[*.seg]\", os.listdir(labels_path)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fsdsfdsfdfd'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sorted(os.listdir(labels_path))\n",
    "# {os.listdir(labels_path)[i]:i for i in os.listdir(labels_path)}\n",
    "import re\n",
    "# {re.match('(*.)[.seg]', i).groups(0):i for i in os.listdir(labels_path)}\n",
    "# {re.match('(?P<name>*.)seg', i, re.DOTALL).groups(0):i for i in os.listdir(labels_path)}\n",
    "\n",
    "re.match('(?P<name>.*)\\.seg', \"sdsfdsfdfd.seg\")['name']\n",
    "root_dict = {re.match('(?P<name>.*)\\.seg', filename)['name'] : filename for filename in os.listdir(labels_path)}\n",
    "# pt_dict = {re.match('(?P<name>.*)\\.seg', filename)['name'] : filename for filename in os.listdir(points_path)}\n",
    "\n",
    "# for k in root_dict:\n",
    "#     root_dict[k] = \n",
    "# {re.match('(?P<name>.*)\\.pts', filename)['name'] : filename for filename in os.listdir(labels_path)}\n",
    "# for key in root_dict:\n",
    "#     root_dict[key] = \n",
    "# root_dict\n",
    "re.match('.*/(?P<name>.*)\\.seg', \"sdfdsf/f/f/fsdsfdsfdfd.seg\")['name']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.stack(arr_x, axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2712, 3), (2712,))"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_x[0].shape, arr_y[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_x, arr_y = [], []\n",
    "for name in os.listdir(labels_path):\n",
    "    label_path = os.path.join(labels_path, name)\n",
    "    pt_path = os.path.join(points_path, re.match('.*/(?P<name>.*)\\.seg', label_path)['name']) + '.pts'\n",
    "    # pt_path = os.path.join(points_path, re.sub('seg', 'pts', label_path))\n",
    "    arr_y.append(genfromtxt(label_path)-1)\n",
    "    arr_x.append(genfromtxt(pt_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2822, 3), (2822,))"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_x[5].shape, arr_y[5].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchvision.datasets.DatasetFolder(root=r'/Users/ishanshastri/Desktop/3D/pointnet/bag_dataset', loader=genfromtxt)\n",
    "\n",
    "class PointDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, arr_x, arr_y):\n",
    "        self.arr_x = arr_x\n",
    "        self.arr_y = arr_y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.arr_y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'x': torch.from_numpy((self.arr_x[idx]).astype(float)),\n",
    "            'y': torch.from_numpy((self.arr_y[idx]).astype(float))\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_point_dataset = PointDataset(arr_x=arr_x, arr_y=arr_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': tensor([[-0.0587,  0.0716,  0.1490],\n",
       "         [ 0.0203, -0.2132,  0.3233],\n",
       "         [ 0.0255, -0.3164,  0.3224],\n",
       "         ...,\n",
       "         [ 0.0388, -0.2028,  0.3196],\n",
       "         [ 0.0418, -0.2234,  0.3188],\n",
       "         [ 0.1038, -0.3267,  0.2927]], dtype=torch.float64),\n",
       " 'y': tensor([0., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float64)}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_point_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8*len(my_point_dataset))#point_dataloader_test\n",
    "train_set, val_set = torch.utils.data.random_split(my_point_dataset, [train_size, len(my_point_dataset)-train_size])\n",
    "\n",
    "point_dataloader_train = torch.utils.data.DataLoader(my_point_dataset, batch_size=1)#32)#torch.utils.data.DataLoader(my_point_dataset[0:int(train_size*len(my_point_dataset))])\n",
    "point_dataloader_val = torch.utils.data.DataLoader(val_set, batch_size=1)#32)#torch.utils.data.DataLoader(my_point_dataset[int(train_size*len(my_point_dataset)):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2661]), torch.Size([2661, 3]))"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_point_dataset[0]['y'].shape, my_point_dataset[0]['x'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_weight = 0.001\n",
    "# def reg_loss(weights):\n",
    "    \n",
    "    # return nn.MSELoss(identity_matrix(dim=weights.shape[0]))\n",
    "\n",
    "def train_epoch(model:PointNetForSegmentation, dataset):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "    # optimizer = torch.optim.adam()\n",
    "    losses_total = []#0\n",
    "    i=0\n",
    "    for pt in dataset:\n",
    "        # print(i)\n",
    "        i+=1\n",
    "        x, y = pt['x'].float(), pt['y'].float()\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x) # y_pred: (N, L, C) ; y: (N, L) (contains labels as class indices)\n",
    "\n",
    "        # Regularization term\n",
    "        # reg_loss = torch.linalg.matrix_norm(\n",
    "        #     torch.eye(n=model.point_net.T2.affine_T.weight.shape[0])\n",
    "        #     -\n",
    "        #     torch.matmul(model.point_net.T2.affine_T.weight, model.point_net.T2.affine_T.weight.T)\n",
    "        #     )\n",
    "\n",
    "        # Loss computation -- we transpose y_pred to compute multi-K cross entropy which expects (N, C, L) \n",
    "        total_loss = loss_fn(y_pred.transpose(-1, -2), y.long()) #s+ reg_weight*reg_loss\n",
    "\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses_total.append(total_loss)\n",
    "\n",
    "    return torch.tensor(losses_total).mean(dim=0), None#torch.mean(torch.tensor(losses_total))#losses_total/len(list(dataset)), \n",
    "        \n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 0., 0., 0., 1., 0., 0., 0., 1.], dtype=torch.float64)\n",
      "Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 0., 0., 0., 1., 0., 0., 0., 1.], requires_grad=True)\n",
      "tensor([1., 0., 0.,  ..., 0., 0., 1.], dtype=torch.float64)\n",
      "Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 0., 0.,  ..., 0., 0., 1.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model = PointNetForSegmentation(num_classes=2, dim=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss: 0, accuracy: tensor([0.7752, 0.9403, 0.6895, 0.6977, 0.7250, 0.7040, 0.6432, 0.6063, 0.5801,\n",
      "        0.7117, 0.5959, 0.7226, 0.5673, 0.5133, 0.5433, 0.5395, 0.5863, 0.5360,\n",
      "        0.4916, 0.7564, 0.5232, 0.4757, 0.6982, 0.8347, 0.7339, 0.5600, 0.6434,\n",
      "        0.7549, 0.6757, 0.6234, 0.5903, 0.5740, 0.6320, 0.7471, 0.5144, 0.6096,\n",
      "        0.5206, 0.6549, 0.5678, 0.4814, 0.6306, 0.5369, 0.4584, 0.4693, 0.4081,\n",
      "        0.8689, 0.8712, 0.5816, 0.4801, 0.4709, 0.6203, 0.6145, 0.8649, 0.8352,\n",
      "        0.6370, 0.4491, 0.7993, 0.5462, 0.6165, 0.7093, 0.7198, 0.6946, 0.6387,\n",
      "        0.5398, 0.6448, 0.5620, 0.5663, 0.5356, 0.6786, 0.6699, 0.4925, 0.6278,\n",
      "        0.6160, 0.5491, 0.5645, 0.4913])\n",
      "epoch 1, loss: 0, accuracy: tensor([0.4862, 0.9370, 0.4874, 0.5175, 0.6780, 0.6686, 0.5206, 0.5069, 0.5055,\n",
      "        0.7329, 0.5142, 0.7216, 0.5226, 0.4987, 0.4611, 0.4833, 0.5187, 0.4764,\n",
      "        0.4413, 0.7957, 0.4779, 0.4223, 0.7255, 1.0271, 0.7561, 0.4769, 0.5900,\n",
      "        0.9270, 0.6766, 0.5987, 0.5504, 0.5565, 0.5829, 0.8109, 0.4848, 0.5663,\n",
      "        0.4984, 0.5975, 0.5176, 0.4606, 0.5710, 0.5008, 0.4428, 0.4400, 0.3945,\n",
      "        0.9162, 0.9075, 0.5476, 0.4608, 0.4634, 0.5803, 0.5792, 0.9043, 0.9013,\n",
      "        0.6296, 0.4370, 0.8716, 0.5173, 0.5705, 0.7362, 0.7335, 0.6757, 0.6103,\n",
      "        0.5144, 0.6577, 0.5421, 0.5377, 0.5262, 0.6767, 0.6496, 0.4741, 0.5987,\n",
      "        0.5932, 0.5332, 0.5470, 0.4715])\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "for i in range(num_epochs):\n",
    "    loss, accuracy = train_epoch(model=model, dataset=point_dataloader_train)\n",
    "    print(f\"epoch {i}, loss: {loss}, accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Profiler didn't finish running",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[169], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m record_function(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_step\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      4\u001b[0m         model(sample_batched_input)\n\u001b[0;32m----> 5\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[43mprof\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_averages\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# model(sample_batched_input)#, sample_batched_input.shape\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/profiler/profiler.py:247\u001b[0m, in \u001b[0;36m_KinetoProfile.key_averages\u001b[0;34m(self, group_by_input_shape, group_by_stack_n)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Averages events, grouping them by operator name and (optionally) input shapes and\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;124;03mstack.\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;124;03m    when creating profiler context manager.\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprofiler\n\u001b[0;32m--> 247\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_averages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup_by_input_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup_by_stack_n\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/autograd/profiler.py:441\u001b[0m, in \u001b[0;36mprofile.key_averages\u001b[0;34m(self, group_by_input_shape, group_by_stack_n)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mkey_averages\u001b[39m(\u001b[38;5;28mself\u001b[39m, group_by_input_shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, group_by_stack_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m--> 441\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_finish\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_events \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected profiling results\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_events\u001b[38;5;241m.\u001b[39mkey_averages(group_by_input_shape, group_by_stack_n)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/autograd/profiler.py:395\u001b[0m, in \u001b[0;36mprofile._check_finish\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_finish\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_events \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 395\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProfiler didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt finish running\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Profiler didn't finish running"
     ]
    }
   ],
   "source": [
    "from torch.profiler import profiler, record_function, ProfilerActivity, profile\n",
    "with profile(activities=[ProfilerActivity.CPU], record_shapes=True, with_stack=True) as prof:\n",
    "    with record_function(\"train_step\"):\n",
    "        model(sample_batched_input)\n",
    "        print(prof.key_averages())\n",
    "# model(sample_batched_input)#, sample_batched_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(1,16))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
